<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0" uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title type="html">Integrating Drools DMN Engine with IBM Open Prediction Service</title><link rel="alternate" href="https://blog.kie.org/2022/04/integrating-drools-dmn-engine-with-ibm-open-prediction-service.html" /><author><name>Matteo Mortari</name></author><id>https://blog.kie.org/2022/04/integrating-drools-dmn-engine-with-ibm-open-prediction-service.html</id><updated>2022-04-14T07:25:47Z</updated><content type="html">In this blog post we’re going to explore an integration between the and another open source project from IBM: "" (OPS). INTRODUCTION Integrating symbolic AIs (rule engines, KRR, etc) with Machine Learning predictive models is an effective strategy to achieve pragmatical, and often more eXplainable, AI solutions. We have also reiterated on this very powerful message across several conferences: For the most recent examples, you can reference or the , or the presentations. This is the reason why we believe the integration between Predictive Models (such as ML or PMML-based solutions) and Decision Models is very, very important. In this context, we will explore how to integrate the Drools DMN Engine with IBM’s Open Prediction Service hub, to achieve a pragmatic AI solution: The Open Prediction Service offers us a broker mechanism between several backends for ML evaluation of Predictive Models. BUILDING THE DEMO In today’s demo, we will develop a simple loan "fast-track" approval service, based on both a Predictive Model to estimate the Risk Score, and a Decision Table in DMN to apply a business policy. For the Risk Score prediction, you can reference on the IBM OPS repository. Using the documentation, also available as Swagger / OpenAPI descriptor, we can identify the Predictive Model input features and output scoring. Now we understand we will need to supply: * the Credit score * the Income * the Loan Amount requested * the number of instalments * and the Rate As output, we can reference the second predictor as a measure of Risk Score in our Decision Model. We can integrate the ML predictive model inside our DMN model to implement the loan "fast-track" approval as usual by defining a BKM node: Then, we can define a Decision Table implementing the business policy for the "fast-track" mechanism: We have completed our modeling activities with DMN and the Predictive Model served via OPS. INVOKING THE OPS SERVICE On the more technical side, to actually integrate OPS evaluation we can follow two options. The first solution could be to use the Quarkus’ RESTEasy client capabilities. For this demo, it’s enough to define the interface of the service: @Path("/predictions") @RegisterRestClient public interface OPSClient { @POST OPSResponse predictions(OPSRequest request); } You can explore the complete code by referencing the repo of the demo . Then, you just need to configure the actual URL for the OPS, for example: # Connect to OPS Server # on quarkus:dev, we use a local Docker run: %dev.quarkus.http.port=0 %dev.org.acme.demo20220330.OPSClient/mp-rest/url=http://localhost:8080 # as default, we are using an app deployed on OpenShift: org.acme.demo20220330.OPSClient/mp-rest/url=https://{your sandbox}.openshiftapps.com You can reference to , for more details about implementing a REST client with Quarkus. INVOKING OPS USING THE JAVA CLIENT SDK As a next step, we can replace the RESTEasy client, with the SDK offered by the OPS itself. In this case, it will be enough to reference the dependency in the Maven pom.xml: &lt;dependency&gt; &lt;groupId&gt;com.ibm.decision&lt;/groupId&gt; &lt;artifactId&gt;ops-client-sdk&lt;/artifactId&gt; &lt;/dependency&gt; Then, we can just replace the RESTEasy client with the OPS’ RunApi, for example: RunApi api = new RunApi(); Prediction prediction = new Prediction(); // ... prediction.setParameters(Arrays.asList( param("creditScore", creditScore), param("income", income), param("loanAmount", loanAmount), param("monthDuration", monthDuration), param("rate", rate) )); PredictionResponse result = api.prediction(prediction); You can explore the complete code by referencing the repo of the demo . RUNNING THE DEMO We will run the demo on the Red Hat Developer OpenShift Sandbox. Remember you can . The free account has some limitations, but they will not block you in replicate this complete solution! As you can see in the picture, first I have deployed the OPS demo on the sandbox (left). Then, I’ve deployed the DMN demo explained in this post, as a Kogito-based application (right). Then, we will have access to the Swagger OpenAPI code generated by the DMN extension of Kogito: As you can see, the REST API is automatically generated for the InputData nodes as defined by the DMN model (Prospect and Loan). Finally, to exercise the demo, we can make use of the automatically generated forms, based on the Swagger OpenAPI: In this case, as we would expect, the total amount would not classify for the fast-track. Then, we can exercise for a different amount value: In this case, beyond the expected improvement in the Risk Score prediction from the ML model, we classify for a "fasttrack" as the policy in the Decision Table prescribes. You can play with different values, showing how the Risk Score prediction is being affected and causing a different final decision. CONCLUSIONS In this post, we have explored integrating a Decision Model using DMN with a ML predictive model. Machine Learning and Decision Models together can provide a pragmatic, and eXplainable, AI solution. Specifically, we have explored integrating the Drools DMN Engine with IBM Open Prediction Service. The advantage of this integration comes from the capability of OPS to interact with several ML providers! Finally, we have deployed the complete demo on the OpenShift Sandbox. What do you think of this integration demo? Questions? Let us know in the comments below! The post appeared first on .</content><dc:creator>Matteo Mortari</dc:creator></entry><entry><title>Generate Helm charts for your Java application using JKube, Part 1</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/04/14/generate-helm-charts-your-java-application-using-jkube-part-1" /><author><name>Rohan Kumar</name></author><id>d5df0b74-748e-4ed5-a419-222174683e62</id><updated>2022-04-14T07:00:00Z</updated><published>2022-04-14T07:00:00Z</published><summary type="html">&lt;p&gt;Tools provided with &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; continually make it easier for &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java&lt;/a&gt; developers to build applications, store them in registries, and deploy images. This article looks at &lt;a href="https://helm.sh/"&gt;Helm charts&lt;/a&gt;, the fundamental tool for building applications on Kubernetes and the &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; cloud service, together with &lt;a href="https://www.eclipse.org/jkube/"&gt;Eclipse JKube&lt;/a&gt;, which is available as a Kubernetes &lt;a href="https://www.eclipse.org/jkube/docs/kubernetes-maven-plugin"&gt;Maven&lt;/a&gt; or &lt;a href="https://www.eclipse.org/jkube/docs/kubernetes-gradle-plugin"&gt;Gradle&lt;/a&gt; plugin.&lt;/p&gt; &lt;p&gt;The article demonstrates how combining Helm with JKube simplifies Kubernetes manifest management for Java projects. You'll learn how JKube makes it possible for Java developers to automatically generate and publish Helm charts (Figure 1).&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/joint.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/joint.png?itok=6_oHUM6t" width="880" height="975" alt="JKube can package a Java application and push it to a Kubernetes cluster using a Helm chart." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: JKube can package a Java application and push it to a Kubernetes cluster using a Helm chart. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Overview of Helm and JKube&lt;/h2&gt; &lt;p&gt;Kubernetes has been here for a while now, and more and more organizations are showing interest in the platform. Java developers are becoming increasingly familiar with packaging their applications into &lt;a href="https://www.docker.com"&gt;Docker&lt;/a&gt; or &lt;a href="https://podman.io"&gt;Podman&lt;/a&gt; &lt;a href="https://developers.redhat.com/topics/containers/"&gt;container&lt;/a&gt; images and publishing them to container registries such as &lt;a href="https://hub.docker.com"&gt;Docker Hub&lt;/a&gt; and &lt;a href="https://quay.io"&gt;Quay.io&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;When working with Kubernetes developers normally need to write Kubernetes configuration files in YAML format. The files can be written manually, of course. But as the complexity of applications increases, it becomes more and more difficult to manage and maintain these Kubernetes manifests.&lt;/p&gt; &lt;p&gt;The Helm project was introduced in 2015-16 to reduce this complexity. It allows Kubernetes users to package Kubernetes manifests in form of &lt;a href="https://helm.sh/docs/topics/charts/"&gt;Helm charts&lt;/a&gt;. A Helm chart is basically a collection of Kubernetes configuration files. It combines pre-configured Kubernetes application resources with corresponding versions into one easily manageable package.&lt;/p&gt; &lt;p&gt;Helm quickly gained popularity and became a graduated project of the &lt;a href="https://www.cncf.io"&gt;Cloud Native Computing Foundation&lt;/a&gt;. Helm is now considered the de facto package manager for Kubernetes. Think of it as equivalent to the &lt;code&gt;rpm -i&lt;/code&gt; command for &lt;a href="https://developers.redhat.com/topics/linux/"&gt;Linux&lt;/a&gt; systems or &lt;code&gt;pip install&lt;/code&gt; for &lt;a href="https://developers.redhat.com/topics/python"&gt;Python&lt;/a&gt;. Helm processes an application packaged as a chart, and deploys the application to Kubernetes. Through a command-line interface (CLI), Helm makes installation, upgrades, fetching dependencies, and configuring deployments on Kubernetes smoother.&lt;/p&gt; &lt;p&gt;Benefits of Helm charts include:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Easy management of Kubernetes resource manifests&lt;/li&gt; &lt;li&gt;Better distribution of Kubernetes manifests across public and private Helm registries&lt;/li&gt; &lt;li&gt;Better management of version history for deployments with features such as &lt;a href="https://helm.sh/docs/helm/helm_rollback/"&gt;Helm Rollback&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;While Helm is useful, it has a learning curve. You can use Helm directly with Java projects, but it requires some effort. It isn't designed with Java developers' workflow in mind.&lt;/p&gt; &lt;p&gt;That's where Eclipse JKube comes in. JKube is a collection of tools and libraries that make your development experience smoother on top of Kubernetes and OpenShift. JKube is available as a Maven or Gradle plugin, and aids Java developers working with cloud-native technologies such as Docker and Kubernetes. JKube provides an end-to-end workflow for Java developers while building container images of applications, generating opinionated manifests, and deploying the images to a Kubernetes cluster. Along with opinionated defaults, JKube also provides a rich set of configuration options via plugin configuration.&lt;/p&gt; &lt;p&gt;Benefits of Eclipse JKube include:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Creation of container images and Kubernetes manifests with little to no configuration&lt;/li&gt; &lt;li&gt;Creation of Helm charts with little to no configuration&lt;/li&gt; &lt;li&gt;Robust operation in both inner and outer loop scenarios&lt;/li&gt; &lt;li&gt;Exposure as plugins via Maven and Gradle (with which Java developers are already familiar)&lt;/li&gt; &lt;li&gt;No external dependency on any CLI (&lt;code&gt;docker&lt;/code&gt;, &lt;code&gt;podman&lt;/code&gt;, &lt;code&gt;kubectl&lt;/code&gt;, &lt;code&gt;helm&lt;/code&gt;, &lt;code&gt;oc&lt;/code&gt;, etc.)&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;This article will specifically focus on generating Helm Charts using JKube. If you're not already familiar with general aspects of JKube usage, please check out &lt;a href="https://developers.redhat.com/blog/2020/01/28/introduction-to-eclipse-jkube-java-tooling-for-kubernetes-and-red-hat-openshift"&gt;Introduction to Eclipse JKube: Java tooling for Kubernetes and Red Hat OpenShift&lt;/a&gt; to get up to speed.&lt;/p&gt; &lt;h2&gt;Generate Helm charts using Eclipse JKube&lt;/h2&gt; &lt;p&gt;Over the rest of this article, you will package a simple Java project into a Helm chart and publish it to a Helm registry using Eclipse JKube. After that, the Java project will be available to any Kubernetes cluster to be installed using the Helm CLI. Then you will install the chart on an OpenShift cluster on the &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;If you already have an existing Java project, you can try adding the plugins in this section to it. Otherwise, you can go to common starter websites such as &lt;a href="https://code.quarkus.io/"&gt;code.quarkus.io&lt;/a&gt; or &lt;a href="https://start.spring.io/"&gt;spring initializer&lt;/a&gt; to generate an application through Maven or Gradle. The sample code and configuration used in this article is available in a &lt;a href="https://github.com/rohankanojia-forks/eclipse-jkube-helm-demo"&gt;Github repository&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;If you're an Apache Maven user, add the OpenShift Maven plugin to your &lt;code&gt;pom.xml&lt;/code&gt; file through the following configuration:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-xml"&gt;&lt;dependency&gt; &lt;groupId&gt;org.eclipse.jkube&lt;/groupId&gt; &lt;artifactId&gt;openshift-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${jkube.version}&lt;/version&gt; &lt;/dependency&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Once you've set up your Java project, you'll need to package it into a container image. You can use OpenShift Maven Plugin's build goal to package the application. This goal uses an OpenShift Source-to-Image (S2I) build, which builds the image in a pod and pushes it to OpenShift's internal container registry:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ./mvnw oc:build&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Once you've built a container image, you can generate Kubernetes manifests using the &lt;code&gt;resource&lt;/code&gt; goal:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ./mvnw oc:resource&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;After running this command, you should be able to see OpenShift manifests generated in the build output directories:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ tree target/classes/META-INF/ target/classes/META-INF/ ├── jkube │ ├── openshift │ │ ├── jkube-helm-maven-deploymentconfig.yml │ │ ├── jkube-helm-maven-route.yml │ │ └── jkube-helm-maven-service.yml │ └── openshift.yml └── resources └── index.html 3 directories, 5 files&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Instead of preparing a Helm chart manually, package these generated OpenShift manifests into a Helm chart by running the following JKube Helm goal:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ./mvnw oc:helm&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;JKube generates a Helm chart without using the Helm CLI. You should be able to see both compressed and decompressed forms of the generated chart in the build output directory:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ls target/jkube-helm-mavejkube-helm-maven-1.0.0-SNAPSHOT-helmshift.tar.gz target/jkube-helm-maven-1.0.0-SNAPSHOT-helmshift.tar.gz $ tree target/jkube/helm/ target/jkube/helm/ └── jkube-helm-maven └── openshift ├── Chart.yaml ├── README.md ├── templates │ ├── jkube-helm-maven-deploymentconfig.yaml │ ├── jkube-helm-maven-route.yaml │ └── jkube-helm-maven-service.yaml └── values.yaml 3 directories, 6 files&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;For more advanced configuration options for Helm chart generation, please check out the &lt;a href="https://www.eclipse.org/jkube/docs/openshift-maven-plugin#jkube:helm"&gt;oc:helm documentation&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Publish Helm charts to Helm repositories&lt;/h2&gt; &lt;p&gt;After you've packaged your application into a Helm chart, you might also want to upload it to a Helm registry. You can upload the chart using the JKube Helm push goal (&lt;code&gt;oc:helm-push&lt;/code&gt; for Maven and &lt;code&gt;ocHelmPush&lt;/code&gt; for Gradle).&lt;/p&gt; &lt;p&gt;First, you need to provide registry details in the plugin configuration. Because this article offers a demo application, we're providing registry details in &lt;code&gt;snapshotRepository&lt;/code&gt;. For a real application, you should use &lt;code&gt;stableRepository&lt;/code&gt;. The sample configuration is:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-xml"&gt; &lt;plugin&gt; &lt;groupId&gt;org.eclipse.jkube&lt;/groupId&gt; &lt;artifactId&gt;openshift-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${jkube.version}&lt;/version&gt; &lt;configuration&gt; &lt;helm&gt; &lt;snapshotRepository&gt; &lt;name&gt;ChartMuseum&lt;/name&gt; &lt;url&gt;http://localhost:8080/api/charts&lt;/url&gt; &lt;type&gt;CHARTMUSEUM&lt;/type&gt; &lt;username&gt;user1&lt;/username&gt; &lt;/snapshotRepository&gt; &lt;/helm&gt; &lt;/configuration&gt; &lt;/plugin&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;After you have configured the registry details, you can run the JKube Helm push goal. The following command uses Maven and provides the registry password on the command line as a property:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ./mvnw oc:helm-push -Djkube.helm.snapshotRepository.password=secret&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can check the complete list of configuration options for pushing Helm charts in the &lt;a href="https://www.eclipse.org/jkube/docs/openshift-maven-plugin#jkube:helm-push"&gt;oc:helm-push documentation&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Deploy a Helm chart to OpenShift with the Helm CLI&lt;/h2&gt; &lt;p&gt;After you've pushed your Helm chart to a Helm registry, you can deploy your application by pulling it and installing the chart via the Helm CLI. This section shows several common commands.&lt;/p&gt; &lt;p&gt;List all available Helm registries:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ helm repo list&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Get the latest chart updates from the registries:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ helm repo update &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Search the Helm repositories for the chart you pushed in the previous section. Because you used the snapshot option, the following command has to include the &lt;code&gt;--devel&lt;/code&gt; option:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ helm search repo chartmusuem --devel NAME CHART VERSION APP VERSION DESCRIPTION chartmusuem/jkube-helm-maven 1.0.0-SNAPSHOT&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Install the chart:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ helm install --generate-name chartmusuem/jkube-helm-maven --devel I0301 22:23:38.971633 37999 request.go:665] Waited for 1.198529433s due to client-side throttling, not priority and fairness, request: GET:https://api.sandbox.openshiftapps.com:6443/apis/security.openshift.io/v1?timeout=32s NAME: jkube-helm-maven-1646153615 LAST DEPLOYED: Tue Mar 1 22:23:40 2022 NAMESPACE: rokumar-dev STATUS: deployed REVISION: 1 TEST SUITE: None&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This command deploys all OpenShift resources contained in this Helm chart to the OpenShift cluster. If you visit the console in the Developer Sandbox, you should see the application deployed in the Topology section (Figure 2). Click &lt;strong&gt;Open URL&lt;/strong&gt; and you should be redirected to your application's page (Figure 3).&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/console.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/console.png?itok=k8ySXqcU" width="1346" height="756" alt="The OpenShift console shows a Helm chart." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2. The OpenShift console shows a Helm chart. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: The OpenShift console shows a Helm chart.&lt;/figcaption&gt;&lt;/figure&gt;&lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/open_1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/open_1.png?itok=ZDmaGbaE" width="1160" height="813" alt="From the OpenShift console, you can open your application." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3. From the OpenShift console, you can open your application. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 3: From the OpenShift console, you can open your application.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;In this article, you learned about the Helm capabilities provided by the Kubernetes Maven and Gradle JKube plugins. Generating and distributing a complete Helm chart for a Java application consists of just adding the desired Maven or Gradle plugin and invoking a few commands. Look for a follow-up article soon where you'll learn about the advanced configuration options provided by Eclipse JKube for generating Helm charts and pushing them to Helm registries.&lt;/p&gt; &lt;p&gt;For more information about Eclipse JKube, please check out these articles:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://developers.redhat.com/blog/2020/08/24/java-development-on-top-of-kubernetes-using-eclipse-jkube"&gt;Java development on top of Kubernetes using Eclipse JKube&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/12/09/get-started-gradle-plugins-eclipse-jkube"&gt;Get started with Gradle plugins for Eclipse JKube&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;You can also check out the &lt;a href="https://www.eclipse.org/jkube"&gt;Eclipse JKube website&lt;/a&gt;. If you like this project, feel free to follow it via these channels:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://stackoverflow.com/questions/tagged/jkube"&gt;StackOverflow&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.youtube.com/channel/UCpU2tjgpfkTVgeDq-DBSV7A"&gt;YouTube&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://twitter.com/jkubeio"&gt;Twitter&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://katacoda.com/jkubeio"&gt;Katakoda&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://gitter.im/eclipse/jkube"&gt;Gitter Chat&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/04/14/generate-helm-charts-your-java-application-using-jkube-part-1" title="Generate Helm charts for your Java application using JKube, Part 1"&gt;Generate Helm charts for your Java application using JKube, Part 1&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Rohan Kumar</dc:creator><dc:date>2022-04-14T07:00:00Z</dc:date></entry><entry><title type="html">DEFEASIBLE REASONING, DROOLS AND TRUTH MAINTENANCE SYSTEM</title><link rel="alternate" href="https://blog.kie.org/2022/04/defeasible-reasoning-drools-and-truth-maintenance-system.html" /><author><name>Nicole Prentzas</name></author><id>https://blog.kie.org/2022/04/defeasible-reasoning-drools-and-truth-maintenance-system.html</id><updated>2022-04-14T01:00:00Z</updated><content type="html">Defeasible reasoning [1] is a field of interest in both Philosophy and Computer Science (the subdiscipline of Artificial Intelligence (AI)). While the philosophical history of the field goes back to Aristotle, AI has only shown interest in it over the last 40 years.  What is called nonmonotonic reasoning in AI is roughly the same as defeasible reasoning in philosophy [2]. Reasoning is the process of deriving conclusions based on existing knowledge, using a problem solving strategy. Non-monotonic reasoning deals with incomplete or uncertain knowledge, where conclusions can be invalidated when adding new information (facts).  Here is an example: Something that looks red to me may justify me in believing that it is red, but if I subsequently learn that the object is illuminated by red light and I know that that can make things look red when they are not, then I cease to be justified in believing that the object is red. A system to deal with non-monotonic knowledge is the Truth Maintenance System (TMS) [3], a problem solver subsystem for reasoning programs, that is concerned with revising sets of beliefs and maintaining the truth in the system when new information contradicts existing information. More information about the TMS and how this is implemented in the Drools engine is provided below. Other formalisms for defeasible reasoning include logic-based approaches like defeasible logic and argumentation. Defeasible logic [4]: created by Donald Nute, is a simple and efficient rule based non-monotonic formalism. The main intuition of the logic is to be able to derive “plausible” conclusions from partial and sometimes conflicting information. A conclusion can be withdrawn when new information is added, hence conclusions are considered tentative. Defeasible logic is useful when we want to express that some statements are “usually” or “most of the time” true, but not strictly always. Example: (in DeLP programming, [5]) flies(X) -&lt; bird(X)   // a bird typically flies (a defeasible rule) bird(X) &lt;- penguin(X)  // a penguin is a bird (a strict rule) ~flies(X) &lt;- penguin(X)  // penguins don’t fly (a strict rule) penguin(tweety) //tweety is a penguin (a fact) Query: flies(tweety) Answer: NO  //because tweety is a penguin and penguins don’t fly In the above example, the first statement (that a bird flies) is not always true and for that reason it is defined as a defeasible rule. So, in the case of penguin(tweety), the query flies(tweety) returns NO because of the strict rule ~flies(X) &lt;- penguin(X) that is stronger than defeasible rule flies(X) -&lt; bird(X). Argumentation: is a principled form of reasoning with conflicting information. It consists of defining arguments, and attacks or preferences between them, and a process of evaluating these arguments to identify plausible conclusions. The previous example in DeLP can be implemented as an argumentation theory, with preferences between arguments denoting priorities, instead of using strict and defeasible rules. More information about this approach is provided in the Conclusions and Further Reading paragraph. Example: rule(r1(X), fly(X), [bird(X)]).   // birds fly rule(r2(X), neg(fly(X)), [penguin(X)]).   // penguins don’t fly rule(f1, bird(tweety), []).  // tweety is a bird rule(f2, penguin(tweety), []).   // tweety is a penguin rule(pr1(X), prefer(r2(X), r1(X)), []).   // r2 is stronger than r1 Query: prove([neg(fly(tweety))],Delta). Answer: Delta = [f2, r2(tweety)]   // Delta = the admissible argument for the query Query: prove([fly(tweety)],Delta. Answer:  // has no solution Drools and Truth Maintenance Systems [3],[7],[8]   The basic function of the Drools engine is to match data to business rules and determine whether and how to execute rules. To ensure that relevant data is applied to the appropriate rules, the Drools engine makes inferences based on existing knowledge and performs the actions based on the inferred information. The Drools engine uses truth maintenance to justify the assertion of data and enforce truthfulness when applying inferred information to rules, to identify inconsistencies and to handle contradictions. In the Drools engine, data is inserted as facts, using either stated (defined with insert()) or logical insertions (defined with insertLogical()). After stated insertions, facts are generally retracted explicitly. After logical insertions, the facts are automatically retracted when no condition supports the logical insertion. A fact that is logically inserted is considered to be justified by the Drools engine. Example: rule “Allow sweets on Saturday” when    $d : DietAssistant ( day == “Saturday” ) then    insertLogical( new AllowSweets( $d ) ); end The fact (AllowSweets($d)) depends on the truth of the “when” clause. When the rule (DietAssistant(day==”Saturday”)) becomes false the fact is automatically retracted. In the Drools engine there is a “simple” implementation of a TMS available and an experimental implementation of a justified TMS (JTMS). JTMS implementation allows a logical insertion to have a positive or a negative label. This allows for contradiction handling. A logical insertion will only exist in the main working memory, as long as there is no conflict in the labeling – i.e. it must be one or more positive labels, and no negative labels. Example: rule “Allow sweets on Saturday” when    $d : DietAssistant ( day == “Saturday” ) then    insertLogical( new AllowSweets( $d ) ); end rule “Do not allow sweets” when    $d : DietAssistant () then    insertLogical( new AllowSweets( $d ), “neg” ); end The above rules are executed in the order given, so first the AllowSweets object is inserted into the working memory, and then, as a result of the “Do not allow sweets” rule execution, the object is retracted, because of the conflict. Limitations of current JTMS: contradiction handling is done at the level of logical insertion, meaning that the entire object is retracted from the memory when a conflict occurs. A new approach to TMS: contradiction handling at the level of a specific property change, so that the object remains in the memory and indicates the property changes that are in conflict. An idea towards this approach is to use a wrapper class that will be responsible to update the properties of the object to be inserted into the working memory, and provide methods for restoring the object’s state, in the case of a conflict. So instead of insertLogical( new AllowSweets($d)) we can use a command-wrapper class and do insertLogical( new Command($d, {property-changes}), with property changes given in the form of pairs (property,value), e.g. {(allowDenySweets,Ture),(freeDay,Wednesday)}. Then, the contradiction handling process will consider the changes inserted in the working memory at the property level. If positive and negative changes occur for a single property, this would be considered a conflict and the state of that particular property will be restored accordingly. Example: public class Person {    private String name;    private String onDiet=”no”;    private String allowDenySweets=”tbd”;    private String freeDay=null; } rule “Rule1: allow sweets, set freeDay Monday” when    $p : Person () then    insertLogical(new Command($p,{(allowDenySweets,”allow”),(freeDay,”Monday”)})); end rule “Rule2: set freeDay Wednesday” when    $p : Person () then    insertLogical(new Command($p,{(freeDay,”Wednesday”)})); end rule “Rule3: do now allow sweets” when    $p : Person () then    insertLogical(new Command($p,{(allowDenySweets,”allow”)}),“neg”)); end Testing: Rules 1, 2 and 3 are activated in the order given. The activation of Rule1 results in the change of two property values of the object p, the property allowDenySweets is set to “allow” and the property freeDay is set to “Monday”. The activation of Rule2 results in the change of the property freeDay to “Wednesday”. The activation of Rule3 will cause a conflict with Rule1, and the changes in the property allowDenySweets. This will result in restoring the value of this property to its original value (before the activation of Rule1, that is the value of “tbd”). The value of the property freeDay will not be affected. Conclusions and Further Reading: Classical methods of knowledge representation and reasoning are based on the assumption that the information available is complete and consistent. However, in many cases, problems or domains, we’ll find incomplete statements or rules, with unknown conditions, and contradictory conclusions. Defeasible reasoning addresses the problem of reasoning under uncertainty, by allowing conclusions to be retracted in the presence of new information. Truth maintenance systems, defeasible logic and argumentation are some approaches towards defeasibility. They are all presented in the previous paragraphs with a short introduction and examples. Gorgias is a general argumentation framework that combines the ideas of preference reasoning and abduction. It was developed as a Prolog meta-interpreter to support a dialectical argumentation process for the development of applications of argumentation. More information can be found in the paper “Gorgias: Applying argumentation” [6]. References: [1] [2] [3] [4] [5] [6] [7] [8] The post appeared first on .</content><dc:creator>Nicole Prentzas</dc:creator></entry><entry><title>Quarkus Newsletter #19 - April</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/quarkus-newsletter-19/&#xA;            " /><author><name>James Cobb (https://twitter.com/insectengine)</name></author><id>https://quarkus.io/blog/quarkus-newsletter-19/</id><updated>2022-04-14T00:00:00Z</updated><published>2022-04-14T00:00:00Z</published><summary type="html">The April newsletter has been sent. It’s full of great new articles like "Kubernetes Native Java with Quarkus" by Jason Greene, John Clingan and Eric Deandrea. Others include:Develop and Deploy Cloud Native Java Applications at Supersonic speed, Build a REST API from the ground up with Quarkus 2.0, Building Databases...</summary><dc:creator>James Cobb (https://twitter.com/insectengine)</dc:creator><dc:date>2022-04-14T00:00:00Z</dc:date></entry><entry><title type="html">DevOpsDays Raleigh 2022 - Talking Architecture Shop (slides)</title><link rel="alternate" href="http://www.schabell.org/2022/04/devopsdays-raleigh-2022-talking-architecture-shop-slides.html" /><author><name>Eric D. Schabell</name></author><id>http://www.schabell.org/2022/04/devopsdays-raleigh-2022-talking-architecture-shop-slides.html</id><updated>2022-04-13T18:00:00Z</updated><content type="html">I've  that I had a talk accepted to the DevOpsDays Raleigh 2022 conference this year.  Today was the day after travel to Raleigh that we got  to chat with a room of super enthusiastic architects. Thanks for the time and lending us your ears. Below you'll find the talk title and abstract along with the slides for your viewing pleasure. My session is from a series called Talking Architecture Shop. This is focusing on architecture research for solutions in the DevOps domain that scale and will be co-presented with my good friend . TALKING ARCHITECTURE SHOP - EXPLORING OPEN SOURCE DEVOPS AT SCALE  You've heard of large scale open source architectures, but have you ever wanted to take a serious look at these real life enterprise DevOps implementations that scale? This session takes attendees on a tour of multiple use cases covering DevOps challenges with hybrid cloud management with GitOps, DevOps in healthcare, and much more. Not only are these architectures interesting, but they are successful real life implementations featuring open source technologies and power many of your own online experiences. The attendee departs this session with a working knowledge of how to map general open source technologies to their solutions. Material covered is available freely online and attendees can use these solutions as starting points for aligning to their own solution architectures. Join us for an hour of power as we talk architecture shop!</content><dc:creator>Eric D. Schabell</dc:creator></entry><entry><title>Manage namespaces in multitenant clusters with Argo CD, Kustomize, and Helm</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/04/13/manage-namespaces-multitenant-clusters-argo-cd-kustomize-and-helm" /><author><name>Saumeya Katyal</name></author><id>b6f7bcb3-322c-4f97-a993-ee3073d528a3</id><updated>2022-04-13T07:00:00Z</updated><published>2022-04-13T07:00:00Z</published><summary type="html">&lt;p&gt;Cluster administrators on &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; need to create namespaces for multiple developer teams and limit their use of resources by provisioning those namespaces with resource quotas and limit ranges. In this article, you'll learn how to automate these tasks with &lt;a href="https://argo-cd.readthedocs.io/en/stable/"&gt;Argo CD&lt;/a&gt;, and how to use either &lt;a href="https://kustomize.io/"&gt;Kustomize&lt;/a&gt; or &lt;a href="https://helm.sh/docs/"&gt;Helm charts&lt;/a&gt; to simplify the process. The result implements the fundamentals of &lt;a href="https://developers.redhat.com/topics/devops"&gt;DevOps&lt;/a&gt; and &lt;a href="https://developers.redhat.com/topics/gitops"&gt;GitOps&lt;/a&gt;, whereby any changes to the repository update the deployed resources.&lt;/p&gt; &lt;p&gt;There are two types of configuration that administrators have to deal with are:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://kubernetes.io/docs/concepts/policy/resource-quotas/#:~:text=A%20resource%20quota%2C%20defined%20by,by%20resources%20in%20that%20namespace"&gt;Resource quota&lt;/a&gt;: A Kubernetes object that controls the amount of CPU or memory consumed by a namespace. This quota can also limit the number of resources that can be created in a namespace.&lt;/li&gt; &lt;li&gt;&lt;a href="https://kubernetes.io/docs/concepts/policy/limit-range/#:~:text=A%20LimitRange%20is%20a%20policy,per%20PersistentVolumeClaim%20in%20a%20namespace."&gt;Limit range&lt;/a&gt;: This is used in Kubernetes along with resource quotas. Although resource quotas control the overall resource consumption of an entire namespace, they do not place any limit on a pod or container within that namespace, so a single pod or container could use up all of the namespace's resources. Limit ranges specify resources available per pod or container.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;While the general principles outlined here apply to any Kubernetes environment, some of the examples in this article assume that you are deploying on &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; and can use its graphical user interface as well as the &lt;code&gt;oc&lt;/code&gt; command-line interface (CLI).&lt;/p&gt; &lt;h2&gt;A simple Argo CD application&lt;/h2&gt; &lt;p&gt;To automate the administrator's configuration tasks, you'll use &lt;a href="https://argo-cd.readthedocs.io/en/stable/"&gt;Argo CD&lt;/a&gt;, a powerful &lt;a href="https://developers.redhat.com/topics/ci-cd"&gt;continuous delivery&lt;/a&gt; tool for Kubernetes resources. Because namespaces and quotas are Kubernetes resources, Argo CD can manage them.&lt;/p&gt; &lt;p&gt;In this section, you'll create a simple Argo CD application from &lt;a href="https://github.com/saumeya/blog-example-repo"&gt;an example in my GitHub repository&lt;/a&gt;. In the sections that follow this one, I'll demonstrate two better ways to create the application using Kustomize and Helm along with Argo CD. But we'll start with a simple example to help you get your bearings with Argo CD.&lt;/p&gt; &lt;ol&gt;&lt;li&gt; &lt;p&gt;Create a Git repository on your system based on the &lt;a href="https://github.com/saumeya/blog-example-repo/tree/main/namespaces-config"&gt;namespaces-config example in my GitHub repository&lt;/a&gt;. The repository comprises the manifest for all namespaces, quotas, and limit ranges for all the teams in the example.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Add &lt;a href="https://redhat-scholars.github.io/argocd-tutorial/argocd-tutorial/04-syncwaves-hooks.html"&gt;Syncwaves&lt;/a&gt; under the &lt;code&gt;annotations&lt;/code&gt; property in these configuration files. Syncwaves is valuable for imposing an order on separate activities. In this case, you need to create a namespace before you can associate a resource quota and limit range to it. The following files show how you can assure that the namespace is created first by assigning a Syncwave of -1, whereas the resource quota and limit range have a Syncwave of 0.&lt;/p&gt; &lt;p&gt;The configuration for a namespace looks like this:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: v1 kind: Namespace metadata: name: dev annotations: argocd.argoproj.io/sync-wave: "-1" labels: argocd.argoproj.io/managed-by: openshift-gitops&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The configuration for a resource quota looks like this:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: v1 kind: ResourceQuota metadata: name: resource-quota namespace: dev annotations: argocd.argoproj.io/sync-wave: "0" spec: hard: pods: "10"&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The configuration for a limit range looks like this:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: v1 kind: LimitRange metadata: name: limits namespace: dev annotations: argocd.argoproj.io/sync-wave: "0" spec: limits: - default: cpu: 200m memory: 512Mi defaultRequest: cpu: 100m memory: 256Mi type: Container&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a id="common-step" name="common-step"&gt;&lt;/a&gt;Log in to your OpenShift cluster as a cluster administrator.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;The Argo CD application controller needs additional permissions to create resource quotas and limit ranges in cluster namespaces. Use &lt;a href="https://docs.openshift.com/container-platform/4.9/authentication/using-rbac.html"&gt;OpenShift cluster roles and cluster role bindings&lt;/a&gt; to grant these permissions to the application controller of the default &lt;code&gt;argocd&lt;/code&gt; instance in the &lt;code&gt;openshift-gitops&lt;/code&gt; namespace. To carry out this step, create a &lt;code&gt;ClusterRole&lt;/code&gt; and &lt;code&gt;ClusterRoleBinding&lt;/code&gt; and apply them to your cluster.&lt;/p&gt; &lt;p&gt;The configuration for a cluster role looks like this:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: # "namespace" omitted since ClusterRoles are not namespaced name: quota-limit-cluster-role rules: - apiGroups: [""] #specifies core api groups resources: ["resourcequotas", "limitranges"] verbs: ["create"]&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Enter the following command to create the &lt;code&gt;ClusterRole&lt;/code&gt; on the cluster:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc create -f &lt;cluster-role-file-name&gt;.yaml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The configuration for a cluster role binding looks like this:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: rbac.authorization.k8s.io/v1 # This cluster role binding allows Service Account to create resource quotas and limit ranges in any namespace. kind: ClusterRoleBinding metadata: name: create-quota-limit-global roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: quota-limit-cluster-role # Name of cluster role to be referenced subjects: - kind: ServiceAccount name: openshift-gitops-argocd-application-controller namespace: openshift-gitops&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Enter the following command to create the &lt;code&gt;ClusterRoleBinding&lt;/code&gt; on the cluster:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc create -f cluster-role-binding.yaml&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Create an Argo CD Application via the user interface (Figure 1), with the following sample Git repository:&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/app-create.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/app-create.png?itok=PrXZOAAE" width="1409" height="796" alt="In the OpenShift console, you can create an Argo CD Application from this article's example repository." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1. In the OpenShift console, you can create an Argo CD Application from this article's example repository. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: In the OpenShift console, you can create an Argo CD Application from this article's example repository.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;You could also use an &lt;code&gt;Application&lt;/code&gt; custom resource to create an application using the CLI. If you want to take this route, your configuration should look like this:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: namespace-management spec: destination: name: '' namespace: default server: 'https://kubernetes.default.svc' source: path: namespaces-config repoURL: 'https://github.com/saumeya/blog-example-repo.git' targetRevision: HEAD project: default&lt;/code&gt;&lt;/pre&gt; Enter the following command to create the application: &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc create -f &lt;application-cr-file-name&gt;.yaml&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Refresh and synchronize the application to create the namespaces with their related quotas and limit ranges (Figure 2).&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt;&lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/simple-ns-app.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/simple-ns-app.png?itok=tvMIIbdx" width="880" height="581" alt="The Topology view of the OpenShift console shows the Argo CD Application and its limit assignments to other namespaces." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2. The Topology view of the OpenShift console shows the Argo CD Application and its limit assignments to other namespaces. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: The Topology view of the OpenShift console shows the Argo CD application and its limit assignments to other namespaces.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Argo CD automates the assignment of resource limits, but the procedure shown in this example so far requires you to maintain an individual manifest for each team. This repetition of files makes it tedious to manage the configurations, especially if you're dealing with a large number of teams and projects. In the following sections, you'll see some better approaches that make use of Kustomize and Helm.&lt;/p&gt; &lt;h2&gt;Use Kustomize to create and manage resources&lt;/h2&gt; &lt;p&gt;The procedure in this section optimizes some of the tasks in the previous section and leaves others unchanged. By using custom patches in &lt;a href="https://kustomize.io/"&gt;Kustomize&lt;/a&gt;, a Kubernetes-native configuration management tool, you can avoid creating multiple manifests and reuse common elements from resource quotas and limit ranges.&lt;/p&gt; &lt;p&gt;I have placed the relevant files for this example in my &lt;a href="https://github.com/saumeya/blog-example-repo/tree/main/kustomize-namespace-config/teams"&gt;GitHub repository&lt;/a&gt;. The &lt;code&gt;teams&lt;/code&gt; directory defines all the patches for different teams in the base manifests. The associated resources are created when you create the Argo CD Application.&lt;/p&gt; &lt;p&gt;Here is the &lt;code&gt;kustomization.yaml&lt;/code&gt; for a patch:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: team-a bases: - ../../base patches: - target: kind: Namespace name: default-dev patch: |- - op: replace path: /metadata/name value: team-a - target: kind: ResourceQuota name: resource-quota patch: |- - op: replace path: /metadata/name value: quota-team-a - op: replace path: /spec/hard/limits.cpu value: 1 - op: replace path: /spec/hard/services value: 10 - target: kind: LimitRange name: limit-range patch: |- - op: replace path: /metadata/name value: quota-team-a&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here is the &lt;code&gt;kustomization.yaml&lt;/code&gt; file for all the teams:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;bases: - ./team-a - ./team-b - ./team-c&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now follow &lt;a href="#common-step"&gt;steps 3, 4, and 5 from the previous example&lt;/a&gt; to grant additional permissions and create Argo CD Applications to manage your namespaces (Figure 3). Make sure to correctly specify the &lt;code&gt;Path&lt;/code&gt; in step 5 to &lt;code&gt;kustomize-namespace-config/teams&lt;/code&gt;. Refresh and synchronize the application.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/kustomize-ns.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/kustomize-ns.png?itok=8Kpm2WL6" width="715" height="576" alt="The Topology view of the OpenShift console shows the Argo CD Application created by Kustomize and its limit assignments to other namespaces." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3. The Topology view of the OpenShift console shows the Argo CD Application created by Kustomize and its limit assignments to other namespaces. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 3: The Topology view of the OpenShift console shows the Argo CD Application created by Kustomize and its limit assignments to other namespaces.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Use Helm and an ApplicationSet to create and manage resources&lt;/h2&gt; &lt;p&gt;&lt;a href="https://helm.sh/docs/"&gt;Helm templates&lt;/a&gt; can also be used to parameterize configurations for namespaces, resource quotas, and limit ranges. A simple use of Helm charts, however, would require you to create more Argo CD applications to manage these namespaces. To avoid creating multiple applications one by one, use an &lt;a href="https://argo-cd.readthedocs.io/en/stable/user-guide/application-set/"&gt;ApplicationSet&lt;/a&gt; resource to specify the value files and create all the applications in one go.&lt;/p&gt; &lt;ol&gt;&lt;li&gt; &lt;p&gt;Create a Helm chart and add the resource files from &lt;a href="https://github.com/saumeya/blog-example-repo/tree/main/helm-namespace-config"&gt;my example repository&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Here is the &lt;code&gt;templates/namespace.yaml&lt;/code&gt; file:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: v1 kind: Namespace metadata: name: {{ .Values.namespace }} annotations: argocd.argoproj.io/sync-wave: "-1" labels: argocd.argoproj.io/managed-by: openshift-gitops&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here is the &lt;code&gt;templates/limit-range.yaml&lt;/code&gt; file:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: v1 kind: LimitRange metadata: name: limit-range namespace: {{ .Values.namespace }} annotations: argocd.argoproj.io/sync-wave: "0" spec: limits: - default: cpu: {{ .Values.limits.default.cpu | default "200m" }} memory: {{ .Values.limits.default.memory | default "512Mi" }} defaultRequest: cpu: {{ .Values.limits.defaultRequest.cpu | default "100m"}} memory: {{ .Values.limits.defaultRequest.memory | default "256Mi"}} type: {{ .Values.limits.type | default "Container" }}&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here is the &lt;code&gt;templates/resource-quota.yaml&lt;/code&gt; file:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: v1 kind: ResourceQuota metadata: name: resource-quota namespace: {{ .Values.namespace }} annotations: argocd.argoproj.io/sync-wave: "0" spec: hard: requests.cpu: {{ .Values.quota.requests.cpu | default "1"}} requests.memory: {{ .Values.quota.requests.memory | default "1Gi"}} limits.cpu: {{ .Values.quota.limits.cpu | default "2"}} limits.memory: {{ .Values.quota.limits.memory | default "2Gi"}} pods: {{ .Values.quota.pods | default "10"}} persistentvolumeclaims: {{ .Values.quota.persistentvolumeclaims | default "20"}} resourcequotas: {{ .Values.quota.resourcequotas | default "1"}} services: {{ .Values.quota.services | default "5"}}&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;values.yaml&lt;/code&gt; file contains default values. You can create value files with different names and specify those in Argo CD while creating an application:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;# Default values for namespace-app. # This is a YAML-formatted file. # Declare variables to be passed into your templates. namespace: default-dev #specifies the quota to be used for resources quota: requests: cpu: '1' memory: 1Gi limits: cpu: '2' memory: 2Gi pods: "10" persistentvolumeclaims: "20" resourcequotas: "1" services: "5" #specifies the limit ranges for the chart limits: default: memory: 512Mi defaultRequest: cpu: 100m memory: 256Mi type: Container &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here is the &lt;code&gt;Chart.yaml&lt;/code&gt; file:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: v2 name: helm-namespace-config description: A Helm chart for Namespace Management type: application version: 0.1.0 appVersion: "1.16.0"&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Follow &lt;a href="#common-step"&gt;steps 3 and 4&lt;/a&gt; from the first example in this article to grant additional permissions and create Argo CD applications to manage your namespaces.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Now use an &lt;code&gt;ApplicationSet&lt;/code&gt; resource to create multiple applications. Check the &lt;a href="https://github.com/saumeya/blog-example-repo/tree/main/helm-namespace-config/teams"&gt;teams directory&lt;/a&gt; in my example repository, which contains custom value files for various teams. Add the relative paths of these custom value files in the list generator. This procedure generates the applications for specific teams with the required configurations, as illustrated in Figure 4.&lt;/p&gt; &lt;p&gt;Here is the configuration file for the &lt;code&gt;ApplicationSet&lt;/code&gt; resource:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: argoproj.io/v1alpha1 kind: ApplicationSet metadata: name: namespace-config namespace: openshift-gitops spec: generators: - list: elements: - filepath: teams/team-a.yaml name: team-a - filepath: teams/team-b.yaml name: team-b - filepath: teams/team-c.yaml name: team-c template: metadata: name: '{{name}}-namespace-config' spec: project: default syncPolicy: automated: prune: true selfHeal: true syncOptions: - CreateNamespace=true source: repoURL: 'https://github.com/saumeya/blog-example-repo' targetRevision: HEAD path: helm-namespace-config helm: valueFiles: - '{{filepath}}' destination: server: 'https://kubernetes.default.svc' namespace: openshift-gitops&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Apply the &lt;code&gt;ApplicationSet&lt;/code&gt; configuration file to your cluster by running the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc apply -f &lt;application-set-file-name&gt;.yaml&lt;/code&gt;&lt;/pre&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/appset-helm.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/appset-helm.png?itok=Kyu4SeVW" width="1440" height="399" alt="The Application Set created three Applications." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4. The Application Set created three Applications. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 4: The ApplicationSet has created three applications.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;You can now synchronize each application and click on an individual application to see the created resources, as illustrated in Figure 5.&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt;&lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/team-a-guide.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/team-a-guide.png?itok=bMATIYJJ" width="853" height="316" alt="The Topology view of the OpenShift console shows the Team A application created by Helm." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 5. The Topology view of the OpenShift console shows the Team A application created by Helm. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 5: The Topology view of the OpenShift console shows the Team A application created by Helm.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;More information about how to create applications using Helm can be found in &lt;a href="https://argo-cd.readthedocs.io/en/stable/user-guide/helm/"&gt;Argo CD's documentation&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;Conclusion&lt;/h3&gt; &lt;p&gt;This article has shown how to use Argo CD in conjunction with other convenient open source tools to simplify the creation, management, and configuration of Kubernetes namespaces. Whenever you need to change the quotas or limits, all you need to do is modify the configuration files in the source repository and Argo CD does the rest of the work.&lt;/p&gt; &lt;p&gt;If you want to learn more about using Argo CD with Red Hat OpenShift, check out &lt;a href="https://developers.redhat.com/blog/2020/10/01/building-modern-ci-cd-workflows-for-serverless-applications-with-red-hat-openshift-pipelines-and-argo-cd-part-1"&gt;Part 1&lt;/a&gt; and &lt;a href="https://developers.redhat.com/blog/2020/10/14/building-modern-ci-cd-workflows-for-serverless-applications-with-red-hat-openshift-pipelines-and-argo-cd-part-2"&gt;Part 2&lt;/a&gt; of the "Building modern CI/CD workflows for serverless applications with Red Hat OpenShift Pipelines and Argo CD" series on Red Hat Developer.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/04/13/manage-namespaces-multitenant-clusters-argo-cd-kustomize-and-helm" title="Manage namespaces in multitenant clusters with Argo CD, Kustomize, and Helm"&gt;Manage namespaces in multitenant clusters with Argo CD, Kustomize, and Helm&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Saumeya Katyal</dc:creator><dc:date>2022-04-13T07:00:00Z</dc:date></entry><entry><title type="html">Eclipse Vert.x 4.2.7 released!</title><link rel="alternate" href="https://vertx.io/blog/eclipse-vert-x-4-2-7" /><author><name>Julien Viet</name></author><id>https://vertx.io/blog/eclipse-vert-x-4-2-7</id><updated>2022-04-13T00:00:00Z</updated><content type="html">Eclipse Vert.x version 4.2.7 has just been released. It fixes quite a few bugs that have been reported by the community and provides a couple of features</content><dc:creator>Julien Viet</dc:creator></entry><entry><title type="html">Using the default Servlet in WildFly applications</title><link rel="alternate" href="http://www.mastertheboss.com/web/jboss-web-server/using-the-default-servlet-in-wildfly-applications/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/web/jboss-web-server/using-the-default-servlet-in-wildfly-applications/</id><updated>2022-04-12T14:19:06Z</updated><content type="html">This article will teach you how to use the default Servlet available in Undertow Web Server for some concerns, such as directory listing or static resources serving. Most Web servers have a default Servlet available. What’s the use case for a default Servlet? You can use it mainly for two reasons: To serve static resources ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>Observability in 2022: Why it matters and how OpenTelemetry can help</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/04/12/observability-2022-why-it-matters-and-how-opentelemetry-can-help" /><author><name>Ben Evans</name></author><id>058dd256-91a7-4914-b9eb-7cc4fd8a2040</id><updated>2022-04-12T07:00:00Z</updated><published>2022-04-12T07:00:00Z</published><summary type="html">&lt;div class="paragraph"&gt; &lt;p&gt;This article explains the basics of &lt;em&gt;observability&lt;/em&gt; for developers. We’ll look at why observability should interest you, its current level of maturity, and what to look out for to make the most of its potential.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Two years ago, James Governor of the developer analyst firm Redmonk remarked, "Observability is making the transition from being a niche concern to becoming a new frontier for user experience, systems, and service management in web companies and enterprises alike." Today, observability is hitting the mainstream.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;As 2022 gets underway, you should expect to hear more about observability as a concern to take seriously. However, lots of developers are still unsure about what observability actually is—​and some of the descriptions of the subject can be vague and imprecise. Read on to get a foundation in this emerging topic.&lt;/p&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_what_is_observability"&gt;What is observability?&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The discipline of observability grew out of several separate strands of development, including application performance monitoring (APM) and the need to make orchestrated systems such as &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; more comprehensible. Observability aims to provide highly granular insights into the behavior of systems, along with rich context.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Overall, implementing observability is conceptually fairly simple. To enable observability in your projects, you should:&lt;/p&gt; &lt;/div&gt; &lt;div class="olist arabic"&gt; &lt;ol class="arabic"&gt;&lt;li&gt; &lt;p&gt;Instrument systems and applications to collect relevant data (e.g. metrics, traces, and logs).&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Send this data to a separate external system that can store and analyze it.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Provide visualizations and insights into systems as a whole (including query capability for end users).&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt;&lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The final step—​the query and visualization capabilities—​are key to the power of observability.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The theoretical background for the approach comes from system control theory—​essentially asking, "How well can the internal state of a system be inferred from outside?"&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;This requirement has been set down as a constraint in a &lt;a href="https://www.honeycomb.io/blog/so-you-want-to-build-an-observability-tool/"&gt;blog posting by Charity Majors&lt;/a&gt;: "Observability requires that you not have to predefine the questions you will need to ask, or optimize those questions in advance." Meeting this constraint requires the collection of sufficient data to accurately model the system’s internal state.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Incident resolution is a really good fit for observability, and it’s where the practice originated. Site reliability experts (SREs) and &lt;a href="https://developers.redhat.com/topics/devops"&gt;DevOps&lt;/a&gt; teams typically focus on incident response. They are interested in a holistic understanding of complex behavior that replaces fragmentary or pre-judged views based on just one or two pieces of the overall system.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;But if the right data is being collected, the stakeholders for observability are much broader than just SREs, production support, and DevOps folks. Observability gives rise to different goals depending on the stakeholder group using the data.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The promise of a wider use for observability can be seen in some of the discussions about whether observability systems should also collect business-relevant metrics, costs, etc. Such data provides additional context and possible use cases for an observability system, but some practitioners argue that it dilutes the goal of the system.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_why_is_observability_important"&gt;Why is observability important?&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;As applications increasingly move to the cloud, they are becoming more complex. There are typically more services and components in modern applications, with more complex topology as well as a much faster pace of change (driven by practices such as &lt;a href="https://developers.redhat.com/topics/ci-cd"&gt;CI/CD&lt;/a&gt;).&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The growing complexity of applications parallels the increasing popularity of technologies with genuinely new behaviors that were created for the cloud. The highlights here include &lt;a href="https://developers.redhat.com/topics/containers"&gt;containerized&lt;/a&gt; environments, dynamically scaling services (especially Kubernetes), and Function-as-a-Service deployments such as AWS Lambda.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;This new world makes root cause analysis and incident resolution potentially a lot harder, yet the same questions still need answering:&lt;/p&gt; &lt;/div&gt; &lt;div class="ulist"&gt; &lt;ul&gt;&lt;li&gt; &lt;p&gt;What is the overall health of my solution?&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;What is the root cause of errors and defects?&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;What are the performance bottlenecks?&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Which of these problems could impact the user experience?&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt;&lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Observability is therefore at the heart of architecting robust, reliable systems for the cloud. The search for an architectural solution to these questions is perhaps best expressed in a &lt;a href="https://copyconstruct.medium.com/monitoring-and-observability-8417d1952e1c"&gt;posting by Cindy Sridharan&lt;/a&gt;: "Since it’s still not possible to predict every single failure mode a system could potentially run into or predict every possible way in which a system could misbehave, it becomes important that we build systems that can be debugged armed with evidence and not conjecture."&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;This new cloud-native world is especially important to Red Hat users because so much of it is based on open source and open standards. The core runtimes, instrumentation components, and telemetry are all open source, and observability components are managed through industry bodies such as the &lt;a href="https://www.cncf.io"&gt;Cloud Native Computing Foundation&lt;/a&gt; (CNCF).&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_what_data_do_we_need_to_collect"&gt;What data do we need to collect?&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Observability data is often conceptualized in terms of three pillars:&lt;/p&gt; &lt;/div&gt; &lt;div class="ulist"&gt; &lt;ul&gt;&lt;li&gt; &lt;p&gt;Distributed traces&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Metrics and monitoring&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Logs&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt;&lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Although some have questioned the value of this categorization, it is a relatively simple mental model, and so is quite useful for developers who are new to observability. Let’s discuss each pillar in turn.&lt;/p&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_distributed_traces"&gt;Distributed traces&lt;/h3&gt; &lt;div class="paragraph"&gt; &lt;p&gt;A &lt;em&gt;distributed trace&lt;/em&gt; is a record of a single service invocation, usually corresponding to a single request from an individual user. The trace includes the following metadata about each request:&lt;/p&gt; &lt;/div&gt; &lt;div class="ulist"&gt; &lt;ul&gt;&lt;li&gt; &lt;p&gt;Which instance was called&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Which containers they were running on&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Which method was invoked&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;How the request performed&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;What the results were&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt;&lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;In distributed architectures, a single service invocation typically triggers downstream calls to other services. These calls, which contribute to the overall trace, are known as &lt;em&gt;spans&lt;/em&gt;, so a trace forms a tree structure of spans. Each span has associated metadata.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The span perspective corresponds to the &lt;em&gt;extrinsic&lt;/em&gt; view of service calls in traditional monitoring. Distributed traces are used to instrument service calls that are request-response oriented. There are additional difficulties associated with calls that do not fit this pattern that tracing struggles to account for.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_metrics_and_monitoring"&gt;Metrics and monitoring&lt;/h3&gt; &lt;div class="paragraph"&gt; &lt;p&gt;&lt;em&gt;Metrics&lt;/em&gt; are numbers measuring specific activity over a time interval. A metric is typically encoded as a tuple consisting of a timestamp, name, value, and dimensions. The dimensions are a set of key-value pairs that describe additional metadata about the metric. Furthermore, it should be possible for a data storage engine to aggregate values across the dimensions of a particular metric to create a meaningful result.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;There are many examples of metrics across many different aspects of a software system:&lt;/p&gt; &lt;/div&gt; &lt;div class="ulist"&gt; &lt;ul&gt;&lt;li&gt; &lt;p&gt;System metrics (including CPU, memory, and disk usage)&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Infrastructure metrics (e.g., from AWS CloudWatch)&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Application metrics (such as APM or error tracking)&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;User and web tracking scripts (e.g., from Google Analytics)&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Business metrics (e.g., customer sign-ups)&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt;&lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Metrics can be gathered from all of the different levels on which the application operates, from very low-level operating system counters all the way up to human and business-scale metrics. Unlike logs or traces, the data volume of metrics does not scale linearly with request traffic—​the exact relationship varies based on the type of metric being collected.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_logs"&gt;Logs&lt;/h3&gt; &lt;div class="paragraph"&gt; &lt;p&gt;&lt;em&gt;Logs&lt;/em&gt; constitute the third pillar of observability. These are defined as immutable records of discrete events that happen over time. Depending on the implementation, there are basically three types of logs: plain text, structured, and binary format. Not all observability products support all three.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Examples of logs include:&lt;/p&gt; &lt;/div&gt; &lt;div class="ulist"&gt; &lt;ul&gt;&lt;li&gt; &lt;p&gt;System and server logs (syslog)&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Firewall and network system logs&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Application logs (e.g., Log4j)&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Platform and server logs (e.g., Apache, NGINX, databases)&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt;&lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_observability_tools_open_source_offerings_on_the_rise"&gt;Observability tools: Open source offerings on the rise&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The APM/monitoring market segment used to be dominated by proprietary vendors. In response, various &lt;a href="https://developers.redhat.com/topics/open-source"&gt;free and open source&lt;/a&gt; software projects started or were spun out of tech companies. Early examples include &lt;a href="https://prometheus.io"&gt;Prometheus&lt;/a&gt; for metrics, and &lt;a href="https://zipkin.io/"&gt;Zipkin&lt;/a&gt; and &lt;a href="https://www.jaegertracing.io/"&gt;Jaeger&lt;/a&gt; for tracing. In the logging space, the "ELK stack" (&lt;a href="https://www.elastic.co/"&gt;Elasticsearch&lt;/a&gt;, &lt;a href="https://www.elastic.co/logstash/"&gt;Logstash&lt;/a&gt;, and &lt;a href="https://www.elastic.co/kibana/"&gt;Kibana&lt;/a&gt;) gained market share and became popular.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;As software continues to become more complex, more and more resources are required to provide a credible set of instrumentation components. For proprietary observability products, this trend creates duplication and inefficiency. The market has hit an inflection point, and it is becoming more efficient for competing companies to collaborate on an open source core and compete on features further up the stack (as well as on pricing).&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;This historical pattern is not an unusual dynamic for open source, and has shown up as a switch from proprietary to open source driven offerings as this market segment migrates from APM to observability. The move to open source can also be partly attributed to the influence of observability startups that have been fully or partially open source since their inception.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;One key milestone was the merger of the OpenTracing and OpenCensus projects to form &lt;a href="https://opentelemetry.io/"&gt;OpenTelemetry&lt;/a&gt;, a major project within CNCF. The project is still maturing, but is gaining momentum. An increasing number of users are investigating and implementing OpenTelemetry, and this number seems set to grow significantly during 2022. A recent survey from the CNCF showed that 49 percent of users were already using OpenTelemetry, and that number is rising rapidly.&lt;/p&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_the_state_of_opentelemetry"&gt;The state of OpenTelemetry&lt;/h3&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Some developers are still confused by exactly what OpenTelemetry actually is. The project offers a set of standards, formats, client libraries, and associated software components. The standards are explicitly cross-platform and not tied to any particular technology stack.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;OpenTelemetry provides a framework that integrates with open source and commercial products and can collect observability data from apps written in many languages. Because the implementations are open source, they are at varying levels of technical maturity, depending on the interest that OpenTelemetry has attracted in specific language communities.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;From the Red Hat perspective, the &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java/JVM&lt;/a&gt; implementation is particularly relevant, being one of the most mature implementations. Components in other major languages and frameworks, such as &lt;a href="https://developers.redhat.com/topics/dotnet"&gt;.NET&lt;/a&gt;, &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt;, and &lt;a href="https://developers.redhat.com/topics/go"&gt;Go&lt;/a&gt;, are also fairly mature.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The implementations work with applications running on bare metal or virtual machines, but OpenTelemetry overall is definitely a cloud-first technology.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;It’s also important to recognize what OpenTelemetry is &lt;em&gt;not.&lt;/em&gt; It isn’t a data ingest, storage, backend, or visualization component. Such components must be provided either by other open source projects or by vendors to produce a full observability solution.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_observability_vs_apm"&gt;Observability vs. APM&lt;/h3&gt; &lt;div class="paragraph"&gt; &lt;p&gt;You may wonder if observability is just a new and flashier name for application performance monitoring. In fact, observability has a number of advantages for the end user over traditional APM:&lt;/p&gt; &lt;/div&gt; &lt;div class="ulist"&gt; &lt;ul&gt;&lt;li&gt; &lt;p&gt;Vastly reduced vendor lock-in&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Open specification wire protocols&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Open source client components&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Standardized architecture patterns&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Increasing quantity and quality of open source backend components&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt;&lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;In the next two years, you should expect to see a further strengthening of key open source observability projects, as well as market consolidation onto a few segment leaders.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_java_observability"&gt;Java observability&lt;/h3&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Java is hugely important to Red Hat’s users, and there are particular challenges to the adoption of observability in Java stacks. Fundamentally, Java technology was designed for a world where JVMs ran on bare metal in data centers, so the applications don’t easily map to tools designed for containerization.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The world is changing, however. Cloud-native deployments—​especially containers—​are here and being adopted quickly, albeit at varying rates across different parts of the industry.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The traditional Java application lifecycle consists of a number of phases: bootstrap, intense class loading, warmup (with JIT compilation), and finally a long-lived steady state (lasting for days or weeks) with relatively little class loading or JIT. This model is challenged by cloud deployments, where containers might live for much shorter time periods and cluster sizes might be dynamically readjusted.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;As it moves to containers, Java has to ensure that it remains competitive along several key axes, including footprint, density, and startup time. Fortunately, ongoing research and development within OpenJDK is trying to make sure that the platform continues to optimize for these characteristics—​and Red Hat is a key contributor to this work.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;If you’re a Java developer looking to adapt to this new world, the first thing to do is prepare your application or company plan for observability. OpenTelemetry is likely to be the library of choice for many developers for both tracing and metrics. Existing libraries, especially &lt;a href="https://micrometer.io/"&gt;Micrometer&lt;/a&gt;, are also likely to have a prominent place in the landscape. In fact, interoperability with existing components within the Java ecosystem is a key goal for OpenTelemetry.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_status_and_roadmap"&gt;Status and roadmap&lt;/h3&gt; &lt;div class="paragraph"&gt; &lt;p&gt;OpenTelemetry has several subprojects that are at different levels of maturity.&lt;/p&gt; &lt;/div&gt; &lt;div class="ulist"&gt; &lt;ul&gt;&lt;li&gt; &lt;p&gt;The Distributed Tracing specification is at v1.0 and is being widely deployed into production systems. It replaces OpenTracing completely, and the OpenTracing project has officially been &lt;a href="https://medium.com/opentracing/opentracing-has-been-archived-fb2848cfef67"&gt;archived&lt;/a&gt;. The Jaeger project, one of the most popular distributing tracing backends, has also discontinued its client libraries and will default to OpenTelemetry protocols going forward.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;The OpenTelemetry Metrics project is not quite as advanced, but it is approaching v1.0 and General Availability (GA). At time of writing, the protocol is at the Stable stage and the API is at Feature Freeze. It is anticipated that the project might reach v1.0/GA during April 2022.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Finally, the Logging specification is still in Draft stage and is not expected to reach v1.0 until late 2022. There is still a certain acknowledged amount of work to do on the spec, and participation is actively being sought by the working groups.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt;&lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Overall, OpenTelemetry as a whole will be considered to be v1.0/GA when the Metrics standard reaches v1.0 alongside Tracing.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The major takeaways are that observability is reaching more and more developers and is noticeably gathering steam. Some analysts even anticipate that OpenTelemetry formats will become the largest single contributor to observability traffic as early as 2023.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_red_hat_and_the_opentelemetry_collector"&gt;Red Hat and the OpenTelemetry Collector&lt;/h3&gt; &lt;div class="paragraph"&gt; &lt;p&gt;As the industry starts to embrace OpenTelemetry, it’s important to help users decide what to do with all the telemetry data that is being generated. One key piece is the &lt;a href="https://opentelemetry.io/docs/collector/"&gt;OpenTelemetry Collector&lt;/a&gt;. This component runs as a network service that can receive, proxy, and transform data. It enables users to keep data and process it internally, or forward it to a third party.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The Collector helps solve one of the major hurdles to adoption faced by a new standard such as OpenTelemetry: Dealing with legacy applications and with infrastructure that already exists. The Collector can understand and speak to many different legacy protocols and payloads, and can translate them into OpenTelemetry-supported protocols. In turn, these open protocols can be consumed by a vast number of vendors who embrace the specification. This unification of older protocols is a major shift in the observability space, and is going to offer a level of flexibility that we haven’t seen before.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Red Hat is deeply committed to the OpenTelemetry community and has released the OpenTelemetry Collector as a component of the &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt; Container Platform, branded as &lt;a href="https://catalog.redhat.com/software/operators/detail/5ec54a5c78e79e6a879fa271"&gt;OpenShift distributed tracing data collection&lt;/a&gt;. This helps our users take advantage of all the capabilities the OpenTelemetry Collector has to offer in order to provide a better, more open approach to observability.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The core architectural capabilities of the Collector can be summarized as follows:&lt;/p&gt; &lt;/div&gt; &lt;div class="ulist"&gt; &lt;ul&gt;&lt;li&gt; &lt;p&gt;&lt;strong&gt;Usability:&lt;/strong&gt; A reasonable default configuration that works out of the box&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Performance:&lt;/strong&gt; Performant under varying loads and configurations&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Observability:&lt;/strong&gt; A good example of an observable service&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Extensibility:&lt;/strong&gt; Customizable without touching the core code&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Unification:&lt;/strong&gt; A single codebase that supports traces, metrics, and logs&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt;&lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_conclusion"&gt;Conclusion&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Observability is an emerging set of ideas and DevOps practices that help handle the complexity of modern architectures and applications, rather than any specific set of products.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Observability absorbs and extends classic monitoring systems, and helps teams identify the root cause of issues. More broadly, it allows stakeholders to answer questions about their application and business, including forecasting and predictions about what could go wrong.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;A diverse collection of tools and technologies are in use, which leads to a large matrix of possible deployments. This has architectural consequences, so teams need to understand how to set up their observability systems in a way that works for them.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;One key technology is OpenTelemetry. It is rapidly gaining popularity, but is still maturing, and the open source groups and standards need more participation, especially by end users.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/04/12/observability-2022-why-it-matters-and-how-opentelemetry-can-help" title="Observability in 2022: Why it matters and how OpenTelemetry can help"&gt;Observability in 2022: Why it matters and how OpenTelemetry can help&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Ben Evans</dc:creator><dc:date>2022-04-12T07:00:00Z</dc:date></entry><entry><title>Quarkus 2.8.0.Final released - New REST layer by default, GraalVM 22.0 and much more!</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/quarkus-2-8-0-final-released/&#xA;            " /><author><name>Guillaume Smet (https://twitter.com/gsmet_)</name></author><id>https://quarkus.io/blog/quarkus-2-8-0-final-released/</id><updated>2022-04-12T00:00:00Z</updated><published>2022-04-12T00:00:00Z</published><summary type="html">2.8.0.Final comes with a lot of refinements and new features: Move Assertj outside of our BOM New REST layer by default GraalVM 22.0 Support for OIDC Proof Of Key for Code Exchange (PKCE) QuarkusTransaction API Elasticsearch Dev Services And much more! Migration Guide To migrate from 2.7, please refer to...</summary><dc:creator>Guillaume Smet (https://twitter.com/gsmet_)</dc:creator><dc:date>2022-04-12T00:00:00Z</dc:date></entry></feed>
